{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data =  (50000, 32, 32, 3)\n",
      "testing data =  (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# download and split the data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(\"training data = \", x_train.shape)\n",
    "print(\"testing data = \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9619245 , -0.91515917, -0.8995707 ],\n",
       "        [-1.2113398 , -1.1645745 , -1.1801629 ],\n",
       "        [-1.1022207 , -1.1333976 , -1.2113398 ],\n",
       "        ...,\n",
       "        [ 0.58133286,  0.17603296, -0.19809005],\n",
       "        [ 0.48780212,  0.06691375, -0.2916208 ],\n",
       "        [ 0.4254483 ,  0.05132529, -0.27603233]],\n",
       "\n",
       "       [[-1.6322283 , -1.5698744 , -1.5698744 ],\n",
       "        [-1.8816435 , -1.8816435 , -1.8816435 ],\n",
       "        [-1.6010513 , -1.756936  , -1.8816435 ],\n",
       "        ...,\n",
       "        [ 0.03573683, -0.5098592 , -1.0242784 ],\n",
       "        [-0.026617  , -0.5878015 , -1.1022207 ],\n",
       "        [ 0.02014837, -0.52544767, -0.9931014 ]],\n",
       "\n",
       "       [[-1.4919322 , -1.5075206 , -1.554286  ],\n",
       "        [-1.6322283 , -1.7725244 , -1.8816435 ],\n",
       "        [-1.117809  , -1.4607552 , -1.756936  ],\n",
       "        ...,\n",
       "        [-0.04220546, -0.57221305, -1.1022207 ],\n",
       "        [-0.01102854, -0.57221305, -1.1022207 ],\n",
       "        [-0.18250158, -0.7436861 , -1.2269284 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.3607558 ,  0.7683944 , -0.38515157],\n",
       "        [ 1.2516366 ,  0.5033906 , -1.3516359 ],\n",
       "        [ 1.2048712 ,  0.62809825, -1.4763436 ],\n",
       "        ...,\n",
       "        [ 0.6125098 ,  0.19162142, -0.79045147],\n",
       "        [-1.0086899 , -1.3984014 , -1.7725244 ],\n",
       "        [-1.0554553 , -1.3516359 , -1.5698744 ]],\n",
       "\n",
       "       [[ 0.924279  ,  0.28515217, -0.38515157],\n",
       "        [ 0.81515974,  0.03573683, -1.2269284 ],\n",
       "        [ 1.0178097 ,  0.36309445, -1.4139898 ],\n",
       "        ...,\n",
       "        [ 0.9866328 ,  0.4254483 , -0.41632846],\n",
       "        [-0.3695631 , -0.91515917, -1.3516359 ],\n",
       "        [-0.5878015 , -1.0554553 , -1.3516359 ]],\n",
       "\n",
       "       [[ 0.8775136 ,  0.36309445, -0.07338238],\n",
       "        [ 0.7372175 ,  0.12926759, -0.41632846],\n",
       "        [ 0.9086905 ,  0.33191755, -0.52544767],\n",
       "        ...,\n",
       "        [ 1.4854635 ,  0.9866328 ,  0.30074063],\n",
       "        [ 0.4722137 , -0.04220546, -0.57221305],\n",
       "        [ 0.03573683, -0.44750538, -0.75927454]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data to speed up training\n",
    "mean = np.mean(x_train)\n",
    "std = np.std(x_train)\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "# let's look at the normalized values of a sample image\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encode the labels in train and test datasets\n",
    "# we use “to_categorical” function in keras \n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train,num_classes)\n",
    "y_test = to_categorical(y_test,num_classes)\n",
    "\n",
    "# break training set into training and validation sets\n",
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "# let's display one of the one-hot encoded labels\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_10 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ activation_11 (\u001b[38;5;33mActivation\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m20,490\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">309,290</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m309,290\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,394</span> (1.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m308,394\u001b[0m (1.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "# number of hidden units variable \n",
    "# we are declaring this variable here and use it in our CONV layers to make it easier to update from one place\n",
    "base_hidden_units = 32\n",
    "\n",
    "# l2 regularization hyperparameter\n",
    "weight_decay = 1e-4 \n",
    "\n",
    "# instantiate an empty sequential model \n",
    "model = Sequential()\n",
    "\n",
    "# CONV1\n",
    "# notice that we defined the input_shape here because this is the first CONV layer. \n",
    "# we don’t need to do that for the remaining layers\n",
    "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# CONV2\n",
    "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# CONV3\n",
    "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# CONV4\n",
    "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# CONV5\n",
    "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# CONV6\n",
    "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# FC7\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "\n",
    "# compute the data augmentation on the training set\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 1.34468, saving model to model.125epochs.keras\n",
      "704/704 - 35s - 49ms/step - accuracy: 0.3898 - loss: 2.0984 - val_accuracy: 0.5248 - val_loss: 1.3447\n",
      "Epoch 2/125\n",
      "\n",
      "Epoch 2: val_loss improved from 1.34468 to 1.08386, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.5538 - loss: 1.3802 - val_accuracy: 0.6334 - val_loss: 1.0839\n",
      "Epoch 3/125\n",
      "\n",
      "Epoch 3: val_loss improved from 1.08386 to 0.95289, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.6336 - loss: 1.1253 - val_accuracy: 0.6734 - val_loss: 0.9529\n",
      "Epoch 4/125\n",
      "\n",
      "Epoch 4: val_loss improved from 0.95289 to 0.92775, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.6815 - loss: 0.9740 - val_accuracy: 0.6954 - val_loss: 0.9278\n",
      "Epoch 5/125\n",
      "\n",
      "Epoch 5: val_loss improved from 0.92775 to 0.80259, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.7132 - loss: 0.8659 - val_accuracy: 0.7358 - val_loss: 0.8026\n",
      "Epoch 6/125\n",
      "\n",
      "Epoch 6: val_loss improved from 0.80259 to 0.74144, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.7411 - loss: 0.7924 - val_accuracy: 0.7598 - val_loss: 0.7414\n",
      "Epoch 7/125\n",
      "\n",
      "Epoch 7: val_loss improved from 0.74144 to 0.68447, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.7608 - loss: 0.7313 - val_accuracy: 0.7786 - val_loss: 0.6845\n",
      "Epoch 8/125\n",
      "\n",
      "Epoch 8: val_loss improved from 0.68447 to 0.67482, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.7771 - loss: 0.6880 - val_accuracy: 0.7892 - val_loss: 0.6748\n",
      "Epoch 9/125\n",
      "\n",
      "Epoch 9: val_loss improved from 0.67482 to 0.65637, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.7898 - loss: 0.6492 - val_accuracy: 0.7904 - val_loss: 0.6564\n",
      "Epoch 10/125\n",
      "\n",
      "Epoch 10: val_loss improved from 0.65637 to 0.61487, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8003 - loss: 0.6153 - val_accuracy: 0.8104 - val_loss: 0.6149\n",
      "Epoch 11/125\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.61487\n",
      "704/704 - 34s - 49ms/step - accuracy: 0.8116 - loss: 0.5890 - val_accuracy: 0.8042 - val_loss: 0.6322\n",
      "Epoch 12/125\n",
      "\n",
      "Epoch 12: val_loss improved from 0.61487 to 0.57187, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8220 - loss: 0.5586 - val_accuracy: 0.8258 - val_loss: 0.5719\n",
      "Epoch 13/125\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.57187\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8282 - loss: 0.5434 - val_accuracy: 0.8084 - val_loss: 0.6282\n",
      "Epoch 14/125\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.57187\n",
      "704/704 - 33s - 48ms/step - accuracy: 0.8336 - loss: 0.5210 - val_accuracy: 0.8164 - val_loss: 0.5945\n",
      "Epoch 15/125\n",
      "\n",
      "Epoch 15: val_loss improved from 0.57187 to 0.54962, saving model to model.125epochs.keras\n",
      "704/704 - 35s - 49ms/step - accuracy: 0.8440 - loss: 0.5027 - val_accuracy: 0.8354 - val_loss: 0.5496\n",
      "Epoch 16/125\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.54962\n",
      "704/704 - 37s - 52ms/step - accuracy: 0.8491 - loss: 0.4825 - val_accuracy: 0.8238 - val_loss: 0.5686\n",
      "Epoch 17/125\n",
      "\n",
      "Epoch 17: val_loss improved from 0.54962 to 0.54708, saving model to model.125epochs.keras\n",
      "704/704 - 34s - 48ms/step - accuracy: 0.8560 - loss: 0.4653 - val_accuracy: 0.8364 - val_loss: 0.5471\n",
      "Epoch 18/125\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.54708\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8586 - loss: 0.4546 - val_accuracy: 0.8330 - val_loss: 0.5645\n",
      "Epoch 19/125\n",
      "\n",
      "Epoch 19: val_loss improved from 0.54708 to 0.53254, saving model to model.125epochs.keras\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8653 - loss: 0.4395 - val_accuracy: 0.8398 - val_loss: 0.5325\n",
      "Epoch 20/125\n",
      "\n",
      "Epoch 20: val_loss improved from 0.53254 to 0.51624, saving model to model.125epochs.keras\n",
      "704/704 - 33s - 47ms/step - accuracy: 0.8678 - loss: 0.4323 - val_accuracy: 0.8448 - val_loss: 0.5162\n",
      "Epoch 21/125\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.51624\n",
      "704/704 - 34s - 48ms/step - accuracy: 0.8742 - loss: 0.4137 - val_accuracy: 0.8318 - val_loss: 0.5871\n",
      "Epoch 22/125\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 46ms/step - accuracy: 0.8775 - loss: 0.4044 - val_accuracy: 0.8422 - val_loss: 0.5348\n",
      "Epoch 23/125\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 46ms/step - accuracy: 0.8804 - loss: 0.4022 - val_accuracy: 0.8412 - val_loss: 0.5554\n",
      "Epoch 24/125\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 46ms/step - accuracy: 0.8832 - loss: 0.3905 - val_accuracy: 0.8302 - val_loss: 0.5768\n",
      "Epoch 25/125\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8863 - loss: 0.3811 - val_accuracy: 0.8490 - val_loss: 0.5213\n",
      "Epoch 26/125\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8911 - loss: 0.3693 - val_accuracy: 0.8494 - val_loss: 0.5355\n",
      "Epoch 27/125\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 46ms/step - accuracy: 0.8931 - loss: 0.3616 - val_accuracy: 0.8484 - val_loss: 0.5352\n",
      "Epoch 28/125\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8986 - loss: 0.3538 - val_accuracy: 0.8468 - val_loss: 0.5291\n",
      "Epoch 29/125\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 46ms/step - accuracy: 0.8993 - loss: 0.3504 - val_accuracy: 0.8526 - val_loss: 0.5285\n",
      "Epoch 30/125\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.8995 - loss: 0.3465 - val_accuracy: 0.8486 - val_loss: 0.5460\n",
      "Epoch 31/125\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9037 - loss: 0.3391 - val_accuracy: 0.8428 - val_loss: 0.5612\n",
      "Epoch 32/125\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9060 - loss: 0.3340 - val_accuracy: 0.8512 - val_loss: 0.5478\n",
      "Epoch 33/125\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9058 - loss: 0.3344 - val_accuracy: 0.8446 - val_loss: 0.5627\n",
      "Epoch 34/125\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9099 - loss: 0.3209 - val_accuracy: 0.8466 - val_loss: 0.5534\n",
      "Epoch 35/125\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9090 - loss: 0.3242 - val_accuracy: 0.8556 - val_loss: 0.5476\n",
      "Epoch 36/125\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9126 - loss: 0.3153 - val_accuracy: 0.8588 - val_loss: 0.5289\n",
      "Epoch 37/125\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9147 - loss: 0.3111 - val_accuracy: 0.8560 - val_loss: 0.5362\n",
      "Epoch 38/125\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9158 - loss: 0.3089 - val_accuracy: 0.8474 - val_loss: 0.5558\n",
      "Epoch 39/125\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9189 - loss: 0.3009 - val_accuracy: 0.8458 - val_loss: 0.5659\n",
      "Epoch 40/125\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9190 - loss: 0.3003 - val_accuracy: 0.8562 - val_loss: 0.5740\n",
      "Epoch 41/125\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9195 - loss: 0.3031 - val_accuracy: 0.8454 - val_loss: 0.5734\n",
      "Epoch 42/125\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9240 - loss: 0.2904 - val_accuracy: 0.8548 - val_loss: 0.5526\n",
      "Epoch 43/125\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9237 - loss: 0.2928 - val_accuracy: 0.8594 - val_loss: 0.5465\n",
      "Epoch 44/125\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9224 - loss: 0.2914 - val_accuracy: 0.8654 - val_loss: 0.5391\n",
      "Epoch 45/125\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 47ms/step - accuracy: 0.9251 - loss: 0.2862 - val_accuracy: 0.8634 - val_loss: 0.5476\n",
      "Epoch 46/125\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9281 - loss: 0.2783 - val_accuracy: 0.8564 - val_loss: 0.5570\n",
      "Epoch 47/125\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9274 - loss: 0.2824 - val_accuracy: 0.8580 - val_loss: 0.5484\n",
      "Epoch 48/125\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9295 - loss: 0.2775 - val_accuracy: 0.8630 - val_loss: 0.5340\n",
      "Epoch 49/125\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9289 - loss: 0.2781 - val_accuracy: 0.8604 - val_loss: 0.5614\n",
      "Epoch 50/125\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9305 - loss: 0.2740 - val_accuracy: 0.8628 - val_loss: 0.5527\n",
      "Epoch 51/125\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9313 - loss: 0.2738 - val_accuracy: 0.8576 - val_loss: 0.5549\n",
      "Epoch 52/125\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9331 - loss: 0.2691 - val_accuracy: 0.8596 - val_loss: 0.5553\n",
      "Epoch 53/125\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9328 - loss: 0.2688 - val_accuracy: 0.8588 - val_loss: 0.5727\n",
      "Epoch 54/125\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9336 - loss: 0.2678 - val_accuracy: 0.8506 - val_loss: 0.6023\n",
      "Epoch 55/125\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9344 - loss: 0.2624 - val_accuracy: 0.8542 - val_loss: 0.5977\n",
      "Epoch 56/125\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9361 - loss: 0.2618 - val_accuracy: 0.8586 - val_loss: 0.5821\n",
      "Epoch 57/125\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9370 - loss: 0.2583 - val_accuracy: 0.8610 - val_loss: 0.5679\n",
      "Epoch 58/125\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9357 - loss: 0.2627 - val_accuracy: 0.8668 - val_loss: 0.5545\n",
      "Epoch 59/125\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9385 - loss: 0.2589 - val_accuracy: 0.8588 - val_loss: 0.5731\n",
      "Epoch 60/125\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9372 - loss: 0.2594 - val_accuracy: 0.8614 - val_loss: 0.5687\n",
      "Epoch 61/125\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9400 - loss: 0.2566 - val_accuracy: 0.8638 - val_loss: 0.5689\n",
      "Epoch 62/125\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9396 - loss: 0.2552 - val_accuracy: 0.8616 - val_loss: 0.5623\n",
      "Epoch 63/125\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9386 - loss: 0.2579 - val_accuracy: 0.8658 - val_loss: 0.5586\n",
      "Epoch 64/125\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9406 - loss: 0.2539 - val_accuracy: 0.8670 - val_loss: 0.5566\n",
      "Epoch 65/125\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9405 - loss: 0.2532 - val_accuracy: 0.8684 - val_loss: 0.5528\n",
      "Epoch 66/125\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9435 - loss: 0.2489 - val_accuracy: 0.8624 - val_loss: 0.5791\n",
      "Epoch 67/125\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9418 - loss: 0.2540 - val_accuracy: 0.8676 - val_loss: 0.5590\n",
      "Epoch 68/125\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9414 - loss: 0.2540 - val_accuracy: 0.8662 - val_loss: 0.5677\n",
      "Epoch 69/125\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9423 - loss: 0.2500 - val_accuracy: 0.8564 - val_loss: 0.6041\n",
      "Epoch 70/125\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9428 - loss: 0.2485 - val_accuracy: 0.8708 - val_loss: 0.5636\n",
      "Epoch 71/125\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9446 - loss: 0.2478 - val_accuracy: 0.8630 - val_loss: 0.5646\n",
      "Epoch 72/125\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9457 - loss: 0.2471 - val_accuracy: 0.8600 - val_loss: 0.6015\n",
      "Epoch 73/125\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9441 - loss: 0.2470 - val_accuracy: 0.8536 - val_loss: 0.6045\n",
      "Epoch 74/125\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9441 - loss: 0.2476 - val_accuracy: 0.8626 - val_loss: 0.5892\n",
      "Epoch 75/125\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9465 - loss: 0.2409 - val_accuracy: 0.8606 - val_loss: 0.5869\n",
      "Epoch 76/125\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9465 - loss: 0.2444 - val_accuracy: 0.8632 - val_loss: 0.5939\n",
      "Epoch 77/125\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9453 - loss: 0.2492 - val_accuracy: 0.8602 - val_loss: 0.5933\n",
      "Epoch 78/125\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9470 - loss: 0.2446 - val_accuracy: 0.8642 - val_loss: 0.5764\n",
      "Epoch 79/125\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9467 - loss: 0.2397 - val_accuracy: 0.8646 - val_loss: 0.5838\n",
      "Epoch 80/125\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9462 - loss: 0.2420 - val_accuracy: 0.8660 - val_loss: 0.5837\n",
      "Epoch 81/125\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9468 - loss: 0.2388 - val_accuracy: 0.8670 - val_loss: 0.5967\n",
      "Epoch 82/125\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9492 - loss: 0.2365 - val_accuracy: 0.8750 - val_loss: 0.5596\n",
      "Epoch 83/125\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9501 - loss: 0.2337 - val_accuracy: 0.8686 - val_loss: 0.5768\n",
      "Epoch 84/125\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9506 - loss: 0.2356 - val_accuracy: 0.8684 - val_loss: 0.5660\n",
      "Epoch 85/125\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9498 - loss: 0.2358 - val_accuracy: 0.8620 - val_loss: 0.5886\n",
      "Epoch 86/125\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9511 - loss: 0.2330 - val_accuracy: 0.8638 - val_loss: 0.5890\n",
      "Epoch 87/125\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9515 - loss: 0.2328 - val_accuracy: 0.8638 - val_loss: 0.5966\n",
      "Epoch 88/125\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9502 - loss: 0.2311 - val_accuracy: 0.8646 - val_loss: 0.5925\n",
      "Epoch 89/125\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9508 - loss: 0.2359 - val_accuracy: 0.8634 - val_loss: 0.5907\n",
      "Epoch 90/125\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9497 - loss: 0.2377 - val_accuracy: 0.8608 - val_loss: 0.6035\n",
      "Epoch 91/125\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9517 - loss: 0.2318 - val_accuracy: 0.8666 - val_loss: 0.5807\n",
      "Epoch 92/125\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9511 - loss: 0.2327 - val_accuracy: 0.8696 - val_loss: 0.5797\n",
      "Epoch 93/125\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9516 - loss: 0.2312 - val_accuracy: 0.8710 - val_loss: 0.5700\n",
      "Epoch 94/125\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9514 - loss: 0.2344 - val_accuracy: 0.8650 - val_loss: 0.5826\n",
      "Epoch 95/125\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 47ms/step - accuracy: 0.9525 - loss: 0.2315 - val_accuracy: 0.8576 - val_loss: 0.6145\n",
      "Epoch 96/125\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9517 - loss: 0.2350 - val_accuracy: 0.8638 - val_loss: 0.5794\n",
      "Epoch 97/125\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9525 - loss: 0.2328 - val_accuracy: 0.8710 - val_loss: 0.5663\n",
      "Epoch 98/125\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9539 - loss: 0.2311 - val_accuracy: 0.8660 - val_loss: 0.5873\n",
      "Epoch 99/125\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9532 - loss: 0.2328 - val_accuracy: 0.8670 - val_loss: 0.6031\n",
      "Epoch 100/125\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9519 - loss: 0.2326 - val_accuracy: 0.8738 - val_loss: 0.5702\n",
      "Epoch 101/125\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9533 - loss: 0.2300 - val_accuracy: 0.8746 - val_loss: 0.5607\n",
      "Epoch 102/125\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9528 - loss: 0.2312 - val_accuracy: 0.8736 - val_loss: 0.5660\n",
      "Epoch 103/125\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9533 - loss: 0.2310 - val_accuracy: 0.8666 - val_loss: 0.5908\n",
      "Epoch 104/125\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9547 - loss: 0.2280 - val_accuracy: 0.8658 - val_loss: 0.5876\n",
      "Epoch 105/125\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9541 - loss: 0.2277 - val_accuracy: 0.8710 - val_loss: 0.5956\n",
      "Epoch 106/125\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.51624\n",
      "704/704 - 33s - 47ms/step - accuracy: 0.9535 - loss: 0.2297 - val_accuracy: 0.8668 - val_loss: 0.5756\n",
      "Epoch 107/125\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9540 - loss: 0.2303 - val_accuracy: 0.8704 - val_loss: 0.5800\n",
      "Epoch 108/125\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9544 - loss: 0.2269 - val_accuracy: 0.8694 - val_loss: 0.5739\n",
      "Epoch 109/125\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9550 - loss: 0.2271 - val_accuracy: 0.8704 - val_loss: 0.5845\n",
      "Epoch 110/125\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9554 - loss: 0.2245 - val_accuracy: 0.8726 - val_loss: 0.5856\n",
      "Epoch 111/125\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9556 - loss: 0.2257 - val_accuracy: 0.8748 - val_loss: 0.5730\n",
      "Epoch 112/125\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9554 - loss: 0.2268 - val_accuracy: 0.8748 - val_loss: 0.5680\n",
      "Epoch 113/125\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9561 - loss: 0.2241 - val_accuracy: 0.8684 - val_loss: 0.5805\n",
      "Epoch 114/125\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9563 - loss: 0.2259 - val_accuracy: 0.8748 - val_loss: 0.5667\n",
      "Epoch 115/125\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9548 - loss: 0.2262 - val_accuracy: 0.8694 - val_loss: 0.5926\n",
      "Epoch 116/125\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9581 - loss: 0.2204 - val_accuracy: 0.8632 - val_loss: 0.6037\n",
      "Epoch 117/125\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9572 - loss: 0.2242 - val_accuracy: 0.8696 - val_loss: 0.5937\n",
      "Epoch 118/125\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9554 - loss: 0.2251 - val_accuracy: 0.8676 - val_loss: 0.5733\n",
      "Epoch 119/125\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9571 - loss: 0.2212 - val_accuracy: 0.8702 - val_loss: 0.5906\n",
      "Epoch 120/125\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9572 - loss: 0.2236 - val_accuracy: 0.8694 - val_loss: 0.5790\n",
      "Epoch 121/125\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 45ms/step - accuracy: 0.9568 - loss: 0.2243 - val_accuracy: 0.8722 - val_loss: 0.5940\n",
      "Epoch 122/125\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9579 - loss: 0.2209 - val_accuracy: 0.8734 - val_loss: 0.5805\n",
      "Epoch 123/125\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9575 - loss: 0.2245 - val_accuracy: 0.8718 - val_loss: 0.5758\n",
      "Epoch 124/125\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9561 - loss: 0.2268 - val_accuracy: 0.8704 - val_loss: 0.5927\n",
      "Epoch 125/125\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.51624\n",
      "704/704 - 32s - 46ms/step - accuracy: 0.9570 - loss: 0.2227 - val_accuracy: 0.8740 - val_loss: 0.5854\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "batch_size = 64\n",
    "epochs=125\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.125epochs.keras', verbose=1, save_best_only=True)\n",
    "\n",
    "# you can try any of these optimizers by uncommenting the line\n",
    "# optimizer = keras.optimizers.rmsprop(learning_rate=0.001,decay=1e-6)\n",
    "# optimizer = keras.optimizers.adam(learning_rate=0.0005,decay=1e-6)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.0003,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), callbacks=[checkpointer],\n",
    "#                steps_per_epoch=x_train.shape[0] // batch_size, epochs=epochs,verbose=2,\n",
    "#                validation_data=(x_test,y_test))\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_data=(x_valid, y_valid), callbacks=[checkpointer], \n",
    "          verbose=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8626 - loss: 0.6385\n",
      "\n",
      "Test result: 85.980 loss: 0.642\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves of model accuracy\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
